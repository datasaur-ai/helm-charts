dockerHub:
  enabled: true

image:
  registry: 682361690817.dkr.ecr.us-east-1.amazonaws.com/vulcan-ai
  dockerTag: v-6-56-0
  pullPolicy: "IfNotPresent"
  internal: true
services:
  backend:
    replicas: 2
    resources:
      auto: true
      requests:
        cpu: 900m
        memory: 2Gi
      limits:
        cpu: 1000m
        memory: 4Gi
    autoscaling:
      enabled: false
      minReplicas: 2
      maxReplicas: 6
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
  worker:
    critical:
      replicas: 2
      resources:
        auto: true
        requests:
          cpu: 900m
          memory: 2Gi
        limits:
          cpu: 1000m
          memory: 4Gi
      autoscaling:
        enabled: false
        minReplicas: 2
        maxReplicas: 6
        targetCPUUtilizationPercentage: 80
        targetMemoryUtilizationPercentage: 80
    background:
      replicas: 2
      resources:
        auto: true
        requests:
          cpu: 900m
          memory: 2Gi
        limits:
          cpu: 1000m
          memory: 4Gi
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 3
        targetCPUUtilizationPercentage: 80
        targetMemoryUtilizationPercentage: 80
    external:
      replicas: 1
      resources:
        auto: true
        requests:
          cpu: 900m
          memory: 2Gi
        limits:
          cpu: 1000m
          memory: 4Gi
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 3
        targetCPUUtilizationPercentage: 80
        targetMemoryUtilizationPercentage: 80
  database:
    enabled: false
    resources:
      auto: true
      requests:
        cpu: null
        memory: null
      limits:
        cpu: null
        memory: null
    storage: 40G
  frontend:
    resources:
      auto: true
      requests:
        cpu: null
        memory: null
      limits:
        cpu: null
        memory: null
  apacheTike:
    enabled: true
  audiowaveformGenerator:
    enabled: true
  documentConverter:
    enabled: true
  languageTool:
    enabled: true
  presidioAnalyzer:
    enabled: true
  rabbitmq:
    enabled: false
  redis:
    enabled: false
  languageCrunch:
    enabled: false
  coreNlp:
    enabled: false
  sparknlpProvider:
    enabled: false

tls:
  cert: <BASE64ENCODED_CERT>
  key: <BASE64ECNCODED_KEY>

imageCredentials:
  registry: ""
  username: ""
  password: ""

ingress:
  host: <HOSTNAME/DOMAINNAME>
  proxyBodySize: 125m

global:
  image:
    # Change this to image.registry value if cannot use dockerHub (for Grafana)
    registry: null

loki:
  enabled: true
  auth:
    username: null
    password: null
  grafana:
    enabled: true
    grafana.ini:
      server:
        domain: <HOSTNAME/DOMAINNAME/IPADDR>
      smtp:
        enabled: false
        ## Uncomment config below to configure SMTP (https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#smtp)
        # host: <smtp_host>:<smtp_port>
        # user: null
        # password: null
        # cert_file: null
        # key_file: null
        # skip_verify: false
        # from_address: admin@grafana.localhost
        # from_name: Grafana
        # ehlo_identity: grafana
        # startTLS_policy: null (options: "OpportunisticStartTLS", "MandatoryStartTLS", "NoStartTLS")

files: null # Array of (name, content, and mountPath)

env:
  # if you're not comfortable saving credentials on this file, you can use AWS SSM with JSON format
  # move all sensitive information to the AWS SSM, see the installation docs appendix
  USE_AWS_SSM: "false"
  # REQUIRED if USE_AWS_SSM is true
  AWS_SSM_PARAMETER_STORE_NAME: ""
  AWS_SSM_REGION: ""

  PUBLIC_URL: <URL>
  APP_URL: <URL>

  ### REQUIRED VARIABLES ###

  # mariadb indicates app will use the built-in container database
  DATABASE_PORT: "3306"

  # schema
  DATABASE_NAME: "vulcan"

  ENABLE_ACTIONS: "false"
  ENABLE_WHITELISTED_CONFIG: "true"

  # please generate a random string, which will be used to encrypt your data
  ENCRYPTION_KEY: <RANDOM_STRING>
  # change to 1 if you want to use encryption, ignore the above if you don't want encryption
  ENCRYPTION_VERSION: "0"

  # domain name for datasaur
  HOST: <HOSTNAME>

  # please generate a random string for this, which will be used for generating a pre-signed URL
  OBJECT_STORAGE_PRIVATE_SIGNATURE_SALT: d17b12ad-4245-493d-85db-00a9b24bba4c
  # required only for S3 compliant
  # if you're using MinIO, you can use https://localhost:9000
  OBJECT_STORAGE_PRIMARY_ENDPOINT: https://s3.us-east-1.amazonaws.com
  OBJECT_STORAGE_PRIVATE_ENDPOINT: https://s3.us-east-1.amazonaws.com
  # options: AWS_S3, AZURE_BLOB_STORAGE, GOOGLE_CLOUD_STORAGE
  # for all S3 compliant, use AWS_S3
  OBJECT_STORAGE_PRIMARY_CLIENT_NAME: AWS_S3
  OBJECT_STORAGE_PRIVATE_CLIENT_NAME: AWS_S3
  # required only for S3 compliant
  OBJECT_STORAGE_PRIMARY_FORCE_PATH_STYLE: "true"
  OBJECT_STORAGE_PRIVATE_FORCE_PATH_STYLE: "true"
  # required only for S3 compliant
  OBJECT_STORAGE_PRIMARY_REGION: us-east-1
  OBJECT_STORAGE_PRIVATE_REGION: us-east-1
  # you can change the bucket name if you want
  OBJECT_STORAGE_PRIMARY_BUCKET_NAME: <OBJECT_STORAGE_BUCKET_NAME>
  OBJECT_STORAGE_PRIVATE_BUCKET_NAME: <OBJECT_STORAGE_BUCKET_NAME>

  ### OPTIONAL VARIABLES ###

  # fill this variable if you are using assume role to allow your EC2 to access the AWS Resources
  DATASAUR_AWS_ASSUME_ROLE_ARN: ""
  # options: USER, ROLE; make sure the DATASAUR_AWS_ASSUME_ROLE_ARN is properly filled if you use ROLE
  DATASAUR_AWS_PERMISSION_STRATEGY:

  # OPTIONAL
  # if you want to use the on premise object storage, ignore this if you have your own object storage
  # MINIO_ACCESS_KEY and MINIO_SECRET_KEY are the master credentials
  # all object storage access key and secret key should refer to MINIO_ACCESS_KEY and MINIO_SECRET_KEY
  MINIO_ACCESS_KEY: ""
  MINIO_SECRET_KEY: ""

secret:
  # for AWS S3, MinIO, or any S3 compliant, please fill the access key and secret key
  # you can ignore this only if you use ROLE for DATASAUR_AWS_PERMISSION_STRATEGY
  # for Azure Blob Storage, please also fill these variables; access key equals storage account name
  # for Google Cloud Storage, you can ignore both acccess key and secret key
  OBJECT_STORAGE_PRIMARY_ACCESS_KEY: <access_key>
  OBJECT_STORAGE_PRIMARY_SECRET_KEY: <secret_key>
  OBJECT_STORAGE_PRIVATE_ACCESS_KEY: <access_key>
  OBJECT_STORAGE_PRIVATE_SECRET_KEY: <secret_key>

  # MariaDB credentials
  DATABASE_USERNAME: <CREDENTIALS>
  DATABASE_PASSWORD: <CREDENTIALS>

  # RabbitMQ credentials
  RABBITMQ_USERNAME: <CREDENTIALS>
  RABBITMQ_PASSWORD: <CREDENTIALS>
